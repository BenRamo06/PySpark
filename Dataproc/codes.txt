gcloud dataproc clusters create mjtelco \
--project qwiklabs-gcp-01-9f0e1afce29b \
--bucket qwiklabs-gcp-01-9f0e1afce29b \
--region us-central1 \
--zone us-central1-a \
--master-machine-type n1-standard-4 \
--worker-machine-type n1-standard-2 \
--num-masters 1 \
--num-workers 2 \
--master-boot-disk-size 500 \
--worker-boot-disk-size 500 \
--image-version 2.0-debian10 \
--optional-components JUPYTER \
--enable-component-gateway \
--initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh


------------------------------------------
------- With parameters --> OK -------
------------------------------------------

gcloud dataproc jobs submit pyspark \
--cluster mjtelco \
--id mjtelco-test-4 \
--region us-central1 \
--max-failures-per-hour 1 \
gs://qwiklabs-gcp-01-9f0e1afce29b/benchmark.py -- 220
  

------------------------------------------
--------- With parameters --> Â¿?-------
------------------------------------------


gcloud dataproc jobs submit pyspark \
       --cluster sparktodp \
       --region us-central1 \
       spark_analysis.py \
       -- --bucket=$1



-------------------------------------------------
---------------- delete cluster -----------------
-------------------------------------------------

gcloud dataproc clusters delete mjtelco \
--region us-central1

-------------------------------------------------
---------------- Update cluster -----------------
-------------------------------------------------


gcloud dataproc clusters update mjtelco \
--region us-central1 \
--num-workers 5







pip install -r requirements.txt

ROLE=$(/usr/share/google/get_metadata_value attributes/dataproc-role)
if [[ "${ROLE}" == 'Master' ]]; then
  ... master specific actions ...
else
  ... worker specific actions ...
fi


if __name__ == "__main__":
    if len(sys.argv) < 3:
        sys.exit("python submit_job.py project_id region cluster_name")

    project_id = sys.argv[1]
    region = sys.argv[2]
    cluster_name = sys.argv[3]