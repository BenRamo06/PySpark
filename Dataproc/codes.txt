glcoud dataproc clusters create <CLUSTER_NAME> \
--initialization-actions gs://<BUCKET>/file.sh \  --> install components
--num-masters 3  \
--num-workers 2


gcloud dataproc clusters create <CLUSTER_NAME> \
--project qwiklabs-gcp-00-4388aa20a1d9 \
--region us-central1 \  
--zone us-central1-a \  
--image-version 2.0-debian10 
--master-machine-type n1-standard-4 \
--worker-machine-type n1-standard-4 \
--num-masters 3  \
--num-workers 2  \
--optional-components JUPYTER \
--enable-component-gateway  \
--initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh




  


#!/bin/bash
gcloud dataproc jobs submit pyspark \
       --cluster sparktodp \
       --region us-central1 \
       spark_analysis.py \
       -- --bucket=$1



gcloud dataproc clusters deleted <CLUSTER_NAME>
--region <REGION>



-- Set log level
	* gcloud command
		gcloud dataproc jobs submit hadoop --driver-log-levels
	* Spark context
		spark.sparkContext.setLogLevel("DEBUG")


pip install -r requirements.txt

ROLE=$(/usr/share/google/get_metadata_value attributes/dataproc-role)
if [[ "${ROLE}" == 'Master' ]]; then
  ... master specific actions ...
else
  ... worker specific actions ...
fi


if __name__ == "__main__":
    if len(sys.argv) < 3:
        sys.exit("python submit_job.py project_id region cluster_name")

    project_id = sys.argv[1]
    region = sys.argv[2]
    cluster_name = sys.argv[3]