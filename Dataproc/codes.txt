glcoud dataproc clusters create <CLUSTER_NAME> \
--initialization-actions gs://<BUCKET>/file.sh \  --> install components
--num-masters 3  \
--num-workers 2


gcloud dataproc clusters create sparktodp 
--enable-component-gateway 
--region us-central1 
--zone us-central1-a 
--master-machine-type n1-standard-4 
--master-boot-disk-size 500 
--num-workers 2 
--worker-machine-type n1-standard-4 
--worker-boot-disk-size 500 
--image-version 2.0-debian10 
--optional-components JUPYTER 
--project qwiklabs-gcp-00-4388aa20a1d9


gcloud beta dataproc clusters create ${CLUSTER_NAME} \
  --region=${REGION} \
  --zone=${ZONE} \
  --image-version=1.5 \
  --master-machine-type=n1-standard-4 \
  --worker-machine-type=n1-standard-4 \
  --bucket=${BUCKET_NAME} \
  --optional-components=JUPYTER \
  --enable-component-gateway \
  --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh


#!/bin/bash
gcloud dataproc jobs submit pyspark \
       --cluster sparktodp \
       --region us-central1 \
       spark_analysis.py \
       -- --bucket=$1



gcloud dataproc clusters deleted <CLUSTER_NAME>
--region <REGION>



-- Set log level
	* gcloud command
		gcloud dataproc jobs submit hadoop --driver-log-levels
	* Spark context
		spark.sparkContext.setLogLevel("DEBUG")


pip install -r requirements.txt




if __name__ == "__main__":
    if len(sys.argv) < 3:
        sys.exit("python submit_job.py project_id region cluster_name")

    project_id = sys.argv[1]
    region = sys.argv[2]
    cluster_name = sys.argv[3]