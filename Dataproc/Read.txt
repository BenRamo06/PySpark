### HADDOP

What is a cluster
	Cluster is a group of interconnected computers (know as nodes) thant can work together on the same problem.


Spark (Processing Memory, batch and streaming)
	Bounded - DataFrames
	UnBounded - DStreams


Data Proc (Compute and Storage separated)

	Best Practices

		- Use GCS rather than HDFS
		- DataProc and GCS must be in the same region or near for Latency purpose
		- If we have more than 10K input file. We need to create files with more capacity
		- Use Pereemptible VMs lower cost
		- Create cluster with mix of VMS and PVMS

	Benefits
		- Low cost  --> 1 c per virtual cpu per cluster per hour
		- Fast 		--> to start, scale and shut down near 90 seconds or less on average  
		- Reizable  --> Cluster can be created and scaled quickly
		- Open Source --> You can use hadoop or spark, because Dataproc manages updates
		- Versioning --> Allows you to switch different version of hadoop or any tool (spark, hive, pig)
		- High Availability -->
		- Fully managed --> You can't worry for mantaining 	



Hadoop (Processing Disk, batch, Compute and Storage together, Master/Slave Architecture)


	Storage --> HDFS (Hadoop Distruibed File System)
					- Block Structured file system
					- Files is divided into blocks across cluster
					- 128 MB is default configuration block size

					- What if a data node crashes?
					  Do we lose this piece of data?
					  When a block is created it is replicated and stored on different data nodes (Replication Method/Factor, 3 nodes replication by default)
					  Hence, HDFS doesn't lose data even if a node crashed (FAIL TOLERANT)
					  


	Process --> MapReduce
					- Split data in parts to process and every part will be processed separtly  in different data node
					- Has 4 phases:
						
						* Split				= Input is split into fragments (Block)
						* Mapper phases		= Each block will have a map task, where it will process a set of key-value pair.
						* Shuffle and sort  = Key value pair is sorted and grouped in 
						* Reduce phase 		= Reduce task per Key-value group




	Execution --> YARN (Yet another resource negotiator)

					- Manage Resources = RAM, network, bandwith and cpu
					- Resource manager = assign resource
					  Node manager     = handle nodes and monitor resources in the node






	Name Node (Master)
					- Maintains and manages blocks on the DataNodes (slave nodes)
					- Record metadata of the blocks (location(node), size, permissions, hierarchy)
						There are two files associated with the metadata:
							FsImage: It contains the complete state of the file system namespace since the start of the NameNode.
							EditLogs: It contains all the recent modifications made to the file system with respect to the most recent FsImage.
					- Receives a Heartbeat and a block report from all the DataNodes in the cluster to ensure that the DataNodes are live.
					- Responsible to take care of the replication factor of all the blocks
					- In case of the DataNode failure, the NameNode chooses new DataNodes for new replicas,
	Data nodes (Slaves)
					- Store actual data
					- Process data
					- They send heartbeats to the NameNode periodically to report the overall health of HDFS, by default, this frequency is set to 3 seconds.
					


	Secondary NameNode/CheckpointNode:
					- Works concurrently with the primary NameNode as a helper daemon
					- The Secondary NameNode is one which constantly reads all the file systems and metadata from the RAM of the NameNode and writes it into the hard disk or the file system.

	Disventages:

		 it's part of the cluster which means even if you're not running jobs that use the compute hardware on the cluster you still have to pay for that power for the cluster to persist all of that storage this is the disadvantage of tying together compute and storage






When uset it

	HDFS: When you have thousands of partitions and directories, and each file size is relatively small
		  When you modify the HDFS data currently (apppend) or you rename directories
		  You have a lot of partitioned writes (spark.read().write.partitionBy(...).parquet(gs://...))





What is ACID?



Work Tamplates (reusable workflow configuration, for managing and executing workflows. )

	* Managed cluster
		The workflow will create this "ephemeral" cluster to run workflow jobs, and then delete the cluster when the workflow is finished.

	* Cluster selector
		A workflow template can specify an existing cluster on which to run workflow jobs by specifying one or more user labels that were previously applied to one or more clusters. 
		The workflow will run on a cluster that matches all of the specified labels. If multiple clusters match the label(s), 
		Dataproc will select the cluster with the most YARN available memory to run all workflow jobs. 
		At the end of workflow, the selected cluster is not deleted

Cluster long live?
Cluster Ephimeros?  --> Jobs are allocatead as needed and then released and turned down as the job finish




