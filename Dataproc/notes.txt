glcoud dataproc clusters create <CLUSTER_NAME> \
--initialization-actions gs://<BUCKET>/file.sh \  --> install components
--num-masters 3  \
--num-workers 2


#!/bin/bash
gcloud dataproc jobs submit pyspark \
       --cluster sparktodp \
       --region us-central1 \
       spark_analysis.py \
       -- --bucket=$1


gcloud dataproc clusters create sparktodp 
--enable-component-gateway 
--region us-central1 
--zone us-central1-a 
--master-machine-type n1-standard-4 
--master-boot-disk-size 500 
--num-workers 2 
--worker-machine-type n1-standard-4 
--worker-boot-disk-size 500 
--image-version 2.0-debian10 
--optional-components JUPYTER 
--project qwiklabs-gcp-00-4388aa20a1d9





-- Set log level
	* gcloud command
		gcloud dataproc jobs submit hadoop --driver-log-levels
	* Spark context
		spark.sparkContext.setLogLevel("DEBUG")

dasd

i

