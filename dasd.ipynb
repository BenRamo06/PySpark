{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Normal', ['Pidgey', 'Pidgeotto', 'Pidgeot', 'PidgeotMega Pidgeot', 'Rattata', 'Raticate', 'Spearow', 'Fearow', 'Jigglypuff', 'Wigglytuff', 'Meowth', 'Persian', 'Farfetchd', 'Doduo', 'Dodrio', 'Lickitung', 'Chansey', 'Kangaskhan', 'KangaskhanMega Kangaskhan', 'Tauros', 'Ditto', 'Eevee', 'Porygon', 'Snorlax']), ('Ground', ['Sandshrew', 'Sandslash', 'Diglett', 'Dugtrio', 'Cubone', 'Marowak', 'Rhyhorn', 'Rhydon']), ('Fire', ['Charmander', 'Charmeleon', 'Charizard', 'CharizardMega Charizard X', 'CharizardMega Charizard Y', 'Vulpix', 'Ninetales', 'Growlithe', 'Arcanine', 'Ponyta', 'Rapidash', 'Magmar', 'Flareon', 'Moltres']), ('Electric', ['Pikachu', 'Raichu', 'Magnemite', 'Magneton', 'Voltorb', 'Electrode', 'Electabuzz', 'Jolteon', 'Zapdos']), ('Fighting', ['Mankey', 'Primeape', 'Machop', 'Machoke', 'Machamp', 'Hitmonlee', 'Hitmonchan']), ('Rock', ['Geodude', 'Graveler', 'Golem', 'Onix', 'Omanyte', 'Omastar', 'Kabuto', 'Kabutops', 'Aerodactyl', 'AerodactylMega Aerodactyl']), ('Ghost', ['Gastly', 'Haunter', 'Gengar', 'GengarMega Gengar']), ('Ice', ['Jynx', 'Articuno']), ('Bug', ['Caterpie', 'Metapod', 'Butterfree', 'Weedle', 'Kakuna', 'Beedrill', 'BeedrillMega Beedrill', 'Paras', 'Parasect', 'Venonat', 'Venomoth', 'Scyther', 'Pinsir', 'PinsirMega Pinsir']), ('Poison', ['Ekans', 'Arbok', 'Nidoran♀', 'Nidorina', 'Nidoqueen', 'Nidoran♂', 'Nidorino', 'Nidoking', 'Zubat', 'Golbat', 'Grimer', 'Muk', 'Koffing', 'Weezing'])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName('ToDataframe').getOrCreate()\n",
    "\n",
    "\n",
    "# We read a file with minPartitions equal 3\n",
    "\n",
    "rdd_data = spark.sparkContext.textFile('inputs/pokemon.csv',minPartitions=5).zipWithIndex()\n",
    "\n",
    "rdd_filter = rdd_data.filter(lambda x: x[1] >= 1) \\\n",
    "                     .map(lambda x: x[0].split(\",\")) \\\n",
    "                     .filter(lambda x: int(x[11])==1)\n",
    "\n",
    "rdd_get = rdd_filter.map(lambda x: ( x[2],x[1] ) )\n",
    "\n",
    "out = rdd_get.groupByKey().mapValues(list)\n",
    "\n",
    "print(out.take(10))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "+-----+---------+------+--------+\n",
      "|id   |dept     |salary|location|\n",
      "+-----+---------+------+--------+\n",
      "|36636|Finance  |3000  |USA     |\n",
      "|40288|Finance  |5000  |IND     |\n",
      "|42114|Sales    |3900  |USA     |\n",
      "|39192|Marketing|2500  |CAN     |\n",
      "|34534|Sales    |6500  |USA     |\n",
      "+-----+---------+------+--------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- propertiesMap: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+-----+---------+---------------------------------+\n",
      "|id   |dept     |propertiesMap                    |\n",
      "+-----+---------+---------------------------------+\n",
      "|36636|Finance  |{salary -> 3000, location -> USA}|\n",
      "|40288|Finance  |{salary -> 5000, location -> IND}|\n",
      "|42114|Sales    |{salary -> 3900, location -> USA}|\n",
      "|39192|Marketing|{salary -> 2500, location -> CAN}|\n",
      "|34534|Sales    |{salary -> 6500, location -> USA}|\n",
      "+-----+---------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [ (\"36636\",\"Finance\",3000,\"USA\"), \n",
    "    (\"40288\",\"Finance\",5000,\"IND\"), \n",
    "    (\"42114\",\"Sales\",3900,\"USA\"), \n",
    "    (\"39192\",\"Marketing\",2500,\"CAN\"), \n",
    "    (\"34534\",\"Sales\",6500,\"USA\") ]\n",
    "schema = StructType([\n",
    "     StructField('id', StringType(), True),\n",
    "     StructField('dept', StringType(), True),\n",
    "     StructField('salary', IntegerType(), True),\n",
    "     StructField('location', StringType(), True)\n",
    "     ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "#Convert scolumns to Map\n",
    "from pyspark.sql.functions import col,lit,create_map\n",
    "df = df.withColumn(\"propertiesMap\",create_map(\n",
    "        lit(\"salary\"),col(\"salary\"),\n",
    "        lit(\"location\"),col(\"location\")\n",
    "        )).drop(\"salary\",\"location\")\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
